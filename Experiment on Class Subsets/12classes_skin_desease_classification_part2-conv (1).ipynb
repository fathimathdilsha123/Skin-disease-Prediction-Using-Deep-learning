{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1LzDD0c09t3LKeRjFUqQZbcueo4FkqUoE","timestamp":1755246702647},{"file_id":"1Rj-LtoNT-SLV0KzRMVmkzHnWFRM1pCzY","timestamp":1753441712679},{"file_id":"1VaxD13ldqSi4EVSaIarP3sLwOepK7xSz","timestamp":1753198980300}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Load the Data set in to working directory"],"metadata":{"id":"f3AdlMzM1w2w"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrpEfFwA1nU2"},"outputs":[],"source":["# Step 1: Install the Kaggle and KaggleHub libraries\n","# ------------------------------------------------------------------------------\n","# We use pip to install the necessary packages.\n","!pip install -q kaggle kagglehub\n","\n","#==============================================================================\n","#  Part 2: Download the 'Dermnet' dataset using KaggleHub and Save to Drive\n","# ==============================================================================\n","# We will use the modern kagglehub library to download the dataset.\n","# It downloads to a local cache, and then we'll copy it to Google Drive.\n","\n","import kagglehub\n","import os\n","import shutil\n","\n","print(\"Downloading 'Dermnet' dataset with kagglehub...\")\n","# This downloads the dataset to a temporary cache location and returns the path.\n","# The files are automatically unzipped.\n","cached_path = kagglehub.dataset_download(\"shubhamgoel27/dermnet\")\n","print(f\"Dataset downloaded to cache: {cached_path}\")\n","\n","# Define the path in your Google Drive where you want to save the dataset\n","working_path = '/content/Skin_desease_classification/dermnet'\n","\n","os.makedirs(working_path, exist_ok=True)\n","\n","# Now, we copy the files from the cache to your persistent Google Drive folder.\n","print(f\"Copying dataset from cache to your directory at: {working_path}\")\n","\n","# Define the required classes\n","required_classes = [\"Nail Fungus and other Nail Disease\",\n","\"Hair Loss Photos Alopecia and other Hair Diseases\",\n","\"Melanoma Skin Cancer Nevi and Moles\",\n","\"Vasculitis Photos\",\n","\"Acne and Rosacea Photos\",\n","\"Scabies Lyme Disease and other Infestations and Bites\",\n","\"Herpes HPV and other STDs Photos\",\n","\"Vascular Tumors\",\n","\"Warts Molluscum and other Viral Infections\",\n","\"Atopic Dermatitis Photos\",\n","\"Urticaria Hives\",\n","\"Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions\"]\n","\n","\n","# Define the subdirectories within the cached path that contain the image classes\n","subdirs_to_copy_from = [\"test\", \"train\"]\n","\n","for subdir in subdirs_to_copy_from:\n","    source_subdir_path = os.path.join(cached_path, subdir)\n","    destination_subdir_path = os.path.join(working_path, subdir)\n","\n","    # Create the destination subdirectory\n","    os.makedirs(destination_subdir_path, exist_ok=True)\n","\n","    if os.path.isdir(source_subdir_path):\n","        print(f\"Processing subdirectory: {subdir}\")\n","        # Loop through all items within the subdirectory\n","        for item in os.listdir(source_subdir_path):\n","            source_item = os.path.join(source_subdir_path, item)\n","            destination_item = os.path.join(destination_subdir_path, item)\n","\n","            # Only copy if the item is a directory and is in the required_classes list\n","            if os.path.isdir(source_item) and item in required_classes:\n","                print(f\"  Copying class: {item}\")\n","                # Use copytree for directories\n","                shutil.copytree(source_item, destination_item, dirs_exist_ok=True)\n","            elif not os.path.isdir(source_item):\n","                 # Copy files (like metadata files) directly\n","                 shutil.copy2(source_item, destination_item)\n","\n","\n","print(\"\\nSubset of Dataset successfully stored in your working directory!\")"]},{"cell_type":"code","source":["import os\n","import shutil\n","from pathlib import Path\n","\n","\n","def create_subset_of_data(source_dir, destination_dir, num_images_per_class=200):\n","\n","  # Create destination directory if it doesn't exist\n","  os.makedirs(destination_dir, exist_ok=True)\n","\n","  # Loop through each class folder\n","  for class_folder in os.listdir(source_dir):\n","      source_class_path = os.path.join(source_dir, class_folder)\n","      dest_class_path = os.path.join(destination_dir, class_folder)\n","\n","      # Only proceed if it is a directory\n","      if os.path.isdir(source_class_path):\n","          os.makedirs(dest_class_path, exist_ok=True)\n","\n","          # List image files and take the first num_images_per_class\n","          image_files = [f for f in os.listdir(source_class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","          selected_images = image_files[:num_images_per_class]\n","\n","          # Copy selected images\n","          for img_file in selected_images:\n","              src_path = os.path.join(source_class_path, img_file)\n","              dst_path = os.path.join(dest_class_path, img_file)\n","              shutil.copy(src_path, dst_path)\n","\n","  print(\"âœ… Subset creation completed successfully!\")\n"],"metadata":{"id":"goxpJyIC3M7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_files_list(data_path):\n","\n","  classes=os.listdir(data_path)\n","  dic={}\n","  for i in classes:\n","      dic[i]= len(os.listdir(os.path.join(data_path,i)))\n","  print(f\"{'=' * 10} Total Classes {len(dic.keys())} {'=' * 10} \\n\")\n","  for key,value in dic.items():\n","      print(key,\":\",value,\"\\n\")\n","  return dic\n","\n"],"metadata":{"id":"oNlnn6gM5wRa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Source and destination paths for train data\n","source_dir = \"/content/Skin_desease_classification/dermnet/train\"\n","destination_dir = \"/content/Skin_desease_classification/dermnet/sub_dermanet/train\"\n","\n","create_subset_of_data(source_dir, destination_dir, 200)"],"metadata":{"id":"jJRIO1s-4jso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_ = get_files_list(destination_dir)"],"metadata":{"id":"o4RWfQPv5uZh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Source and destination paths for test data\n","source_dir = \"/content/Skin_desease_classification/dermnet/test\"\n","destination_dir = \"/content/Skin_desease_classification/dermnet/sub_dermanet/test\"\n","\n","create_subset_of_data(source_dir, destination_dir, 20)"],"metadata":{"id":"amn6yOwP47Al"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_ = get_files_list(destination_dir)"],"metadata":{"id":"eg0v1Kl_59ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"],"metadata":{"id":"-nqYCVaR2x-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dir = \"/content/Skin_desease_classification/dermnet/sub_dermanet/train\"\n","test_dir = \"/content/Skin_desease_classification/dermnet/sub_dermanet/test\""],"metadata":{"id":"nfxSJXdf6R2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","\n","\n","# visulaizing the images\n","for category in required_classes:\n","  #constructing the path\n","  path = os.path.join(train_dir, category)\n","  images = os.listdir(path)\n","\n","  fig, ax = plt.subplots(1, 3, figsize = (10, 4))\n","  fig.suptitle(f'{category}', fontsize = 18)\n","\n","  for i in range(3):\n","    img_name = images[np.random.randint(0, len(images))]\n","    img_path = os.path.join(path, img_name)\n","    img_array = cv2.imread(img_path)\n","\n","    # converting the BGR images to RGB\n","    img_rgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n","\n","    ax[i].imshow(img_rgb)\n","    ax[i].axis('off')\n"],"metadata":{"id":"UjRc3x7ZRc1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# =============================\n","# CONFIG\n","# =============================\n","img_size = (224, 224)\n","batch_size = 32\n","seed = 42\n","\n","# =============================\n","# Load Datasets\n","# =============================\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    train_dir,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=seed,\n","    image_size=img_size,\n","    batch_size=batch_size\n",")\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    train_dir,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=seed,\n","    image_size=img_size,\n","    batch_size=batch_size\n",")\n","\n","test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    test_dir,\n","    image_size=img_size,\n","    batch_size=batch_size,\n","    shuffle=False\n",")\n","\n","class_names = train_ds.class_names\n","num_classes = len(class_names)\n","\n","# =============================\n","# Compute Class Weights\n","# =============================\n","labels = []\n","for _, label_batch in train_ds.unbatch():\n","    labels.append(label_batch.numpy())\n","\n","class_weights_array = compute_class_weight(\n","    class_weight='balanced',\n","    classes=np.unique(labels),\n","    y=labels\n",")\n","class_weight_dict = dict(enumerate(class_weights_array))\n","\n","# =============================\n","# Augmentation Layer\n","# =============================\n","data_augmentation = tf.keras.Sequential([\n","    tf.keras.layers.RandomFlip(\"horizontal\"),\n","    tf.keras.layers.RandomRotation(0.1),\n","    tf.keras.layers.RandomZoom(0.1),\n","    tf.keras.layers.RandomContrast(0.1),\n","])\n"],"metadata":{"id":"iUFCKHDS6MZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names"],"metadata":{"id":"x0Cs3zOBTArX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =============================\n","# Model Building (Transfer Learning)\n","# =============================\n","from tensorflow.keras.applications import ConvNeXtTiny\n","from tensorflow.keras import layers, models\n","\n","\n","def build_model(trainable_base=False):\n","    base_model = ConvNeXtTiny(include_top=False, weights=\"imagenet\", input_shape=img_size + (3,))\n","    base_model.trainable = trainable_base\n","\n","    inputs = tf.keras.Input(shape=img_size + (3,))\n","    x = data_augmentation(inputs)\n","    x = base_model(x, training=trainable_base)\n","    x = layers.GlobalAveragePooling2D()(x)\n","    x = layers.Dropout(0.4)(x)\n","    outputs = layers.Dense(num_classes, activation='softmax')(x)\n","\n","    model = tf.keras.Model(inputs, outputs)\n","    return model\n","\n","model = build_model(trainable_base=False)\n","\n","# =============================\n","# Compile and Train (Top Layers)\n","# =============================\n","model.compile(\n","    optimizer=tf.keras.optimizers.Adam(),\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n"],"metadata":{"id":"x6thQlpP62_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"dDt-Fp8377Ge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["callbacks = [\n","    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n","    tf.keras.callbacks.ModelCheckpoint(\"base_model_covn_bo_sub200_class12.keras\", save_best_only=True, monitor='val_loss')\n","]\n"],"metadata":{"id":"cpE5CeKJ789P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"ðŸŸ¡ Training top layers...\")\n","history = model.fit(\n","    train_ds.cache().prefetch(tf.data.AUTOTUNE),\n","    validation_data=val_ds.cache().prefetch(tf.data.AUTOTUNE),\n","    epochs=40,\n","    class_weight=class_weight_dict,\n","    callbacks=callbacks\n",")\n"],"metadata":{"id":"sVwno8uF8HAr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def plot_history(history):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    epochs_range = range(len(acc))\n","\n","    plt.figure(figsize=(14, 5))\n","\n","    # Accuracy plot\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs_range, acc, label='Train Accuracy')\n","    plt.plot(epochs_range, val_acc, label='Val Accuracy')\n","    plt.title('Training & Validation Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    # Loss plot\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs_range, loss, label='Train Loss')\n","    plt.plot(epochs_range, val_loss, label='Val Loss')\n","    plt.title('Training & Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Call it with your history\n","plot_history(history)\n"],"metadata":{"id":"GcE1W1Dh8Ta_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get predictions\n","def get_prediction(model):\n","\n","  y_true = []\n","  y_pred = []\n","\n","  for images, labels in test_ds:\n","      preds = model.predict(images)\n","      y_true.extend(labels.numpy())\n","      y_pred.extend(np.argmax(preds, axis=1))\n","\n","  # Confusion matrix\n","  cm = confusion_matrix(y_true, y_pred)\n","  plt.figure(figsize=(12, 10))\n","  sns.heatmap(cm, annot=True, cmap='Blues')\n","  plt.title(\"Confusion Matrix\")\n","  plt.xlabel(\"Predicted\")\n","  plt.ylabel(\"True\")\n","  plt.show()\n","\n","  # Classification report\n","  class_names = test_ds.class_names\n","  print(classification_report(y_true, y_pred, target_names=class_names))\n","\n","# base model prediction\n","get_prediction(model)"],"metadata":{"id":"2J3iuuYS95Pp"},"execution_count":null,"outputs":[]}]}